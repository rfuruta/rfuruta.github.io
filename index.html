<!DOCTYPE html>
<!-- saved from url=(0040)http://www.hal.t.u-tokyo.ac.jp/~furuta/ -->
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Ryosuke Furuta</title>
    <link href="./furuta_files/bootstrap.css" rel="stylesheet">
    <link href="./furuta_files/myedit.css" rel="stylesheet">
</head>
<body>

    <div class="container"> <!-- define by myself -->      
        <div class="text-end">
            <a href="./index_jp.html">日本語</a>/
            <a href="./index.html">English</a>
        </div>
        <br>

        <h2>Ryosuke Furuta</h2>
        <div class="row" style="margin-bottom: 20px; margin-left: 0px;">
            <div class="col-lg-2">
                <img src="photo/furuta.jpg" class="img-max-height" alt="Responsive image">
            </div>
            <div class="col-lg-10">
                <b>Research Associate (Oct. 2020 - present)</b><br>
                <a href="https://www.ut-vision.org/sato-lab/">Y. Sato Laboratory</a><br>
                Institute of Industrial Science, The University of Tokyo, Japan<br>
                E-mail: furuta (at) iis.u-tokyo.ac.jp<br><br>

                <b>Visiting Researcher (Nov. 2020 - present)</b><br>
                <a href="https://www.rs.tus.ac.jp/vml/">Taniguchi Laboratory</a><br>
                Department of Information and Computer Technology, Tokyo University of Science, Japan<br>
            </div>
        </div>

        <h3>Research Interests</h3>
        <p>Optimization for Computer Vision and Image Processing.</p>

        <h3>Work Experience</h3>
        <dl>
            <dt>Assistant Professor (Apr. 2019 - Sep. 2020)</dt>
            <dd>
                <a href="https://www.rs.tus.ac.jp/vml/">Taniguchi Laboratory</a><br>
                Department of Information and Computer Technology, Tokyo University of Science, Japan
            </dd>
            
            <dt>Visiting Researcher (May 2019 - Sep. 2020)</dt>
            <dd>
                <a href="http://www.cvm.t.u-tokyo.ac.jp/en/">Yamasaki Laboratory</a><br>
                Department of Information and Communication Engineering, The University of Tokyo, Japan
            </dd>
        </dl>

        <h3>Education</h3>
        <dl>
            <dt>Ph.D., (Apr. 2016 - Mar. 2019)</dt>
            <dd>
                Department of Information and Communication Engineering,<br>
                Graduate School of Information Science and Technology,<br>
                The University of Tokyo.<br>
                Advisor: <a href="http://www.hal.t.u-tokyo.ac.jp/~yamasaki/index-e.html">Prof. Toshihiko Yamasaki</a>
            </dd>
            <dt>M.S., (Apr. 2014 - Mar. 2016)</dt>
            <dd>
                Department of Information and Communication Engineering,<br>
                Graduate School of Information Science and Technology,<br>
                The University of Tokyo.<br>
                Advisor: <a href="http://www.hal.t.u-tokyo.ac.jp/~yamasaki/index-e.html">Prof. Toshihiko Yamasaki</a>
            </dd>
            <dt>B.E., (Apr. 2010 - Mar. 2014)</dt>
            <dd>
                Department of Information and Communication Engineering,<br>
                The University of Tokyo.<br>
                Advisor: <a href="http://www.hal.t.u-tokyo.ac.jp/~aizawa/">Prof. Kiyoharu Aizawa</a> and <a href="http://www.hal.t.u-tokyo.ac.jp/~yamasaki/index-e.html">Prof. Toshihiko Yamasaki</a>
            </dd>
        </dl>

        <h3 id="publications">Publications</h3>
        <h4>Preprint</h4>
        <ul>
            <li>
                <span class="under">Ryosuke Furuta</span> and Yoichi Sato<br>
                <strong>Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised Domain Generalization for Object Detection</strong><br>
                <i>arXiv, 2024.</i> [<a href="https://arxiv.org/abs/2310.19351">arXiv</a>]
            </li>
        </ul>
        <h4>Journal</h4>
        <ul>
            <li>
                Tomoyuki Hatakeyama, <span class="under">Ryosuke Furuta</span>, and Yoichi Sato<br>
                <strong>Simultaneous Control of Head Pose and Expressions in 3D Facial Keypoint-Based GAN</strong><br>
                <i>MTAP, 2024.</i>
            </li>
        </ul>
        <ul>
            <li>
                Takehiko Ohkawa, <span class="under">Ryosuke Furuta</span>, and Yoichi Sato<br>
                <strong>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</strong><br>
                <i>International Journal of Computer Vision (IJCV), 2023. </i> [<a href="https://arxiv.org/abs/2206.02257">arXiv</a>] [<a href="https://link.springer.com/article/10.1007/s11263-023-01856-0">Springer Link</a>]
            </li>
        </ul>
        <ul>
            <li>
                <span class="under">Ryosuke Furuta</span>, Naoto Inoue, and Toshihiko Yamasaki<br>
                <strong>PixelRL: Fully Convolutional Network with Multi-Step Reinforcement Learning for Image Processing</strong><br>
                <i>IEEE TMM, 2020. (IEEE SPS Japan Young Author Best Paper Award) </i> [<a href="https://arxiv.org/abs/1912.07190">arXiv</a>] [<a href="https://ieeexplore.ieee.org/document/8936404">IEEE Xplore</a>] [<a href="./pub/fcn_rl/fcn_rl.html">project page</a>] [<a href="https://github.com/rfuruta/pixelRL">code</a>]
            </li>
        </ul>
        <ul>
            <li>
                <span class="under">Ryosuke Furuta</span>, Naoto Inoue, and Toshihiko Yamasaki<br>
                <strong>Efficient and Interactive Spatial-Semantic Image Retrieval</strong><br>
                <i>MTAP, 2019.</i> [<a href="https://link.springer.com/article/10.1007/s11042-018-7148-1?wt_mc=Internal.Event.1.SEM.ArticleAuthorOnlineFirst&utm_source=ArticleAuthorOnlineFirst&utm_medium=email&utm_content=AA_en_06082018&ArticleAuthorOnlineFirst_20190204">pdf (open access)</a>]
            </li>
        </ul>
        <ul>
            <li>
                <span class="under">Ryosuke Furuta</span>, Satoshi Ikehata, Toshihiko Yamasaki, and Kiyoharu Aizawa<br>
                <strong>Efficiency-Enhanced Cost-Volume Filtering featuring Coarse-to-Fine Strategy</strong><br>
                <i>MTAP, 2018.</i> [<a href="https://link.springer.com/article/10.1007/s11042-017-4897-1?wt_mc=Internal.Event.1.SEM.ArticleAuthorOnlineFirst">pdf (open access)</a>] [<a href="./pub/c2fcvf/CtF_CVF.zip">code</a>]
            </li>
        </ul>
        <ul>
            <li>
                <span class="under">Ryosuke Furuta</span>, Ikuko Tsubaki, and Toshihiko Yamasaki<br>
                <strong>Fast Volume Seam Carving with Multi-pass Dynamic Programming</strong><br>
                <i>IEEE TCSVT, 2018.</i> (Telecom System Technology Award 2018) [<a href="http://ieeexplore.ieee.org/document/7755760/">pdf (open access)</a>] [<a href="./pub/vsc/result.mp4">video</a>] [<a href="./pub/vsc/suppl.pdf">supplementary</a>]
            </li>
        </ul>
        <h4>International conference</h4>
        <ul>
            <li>
                Nie Lin, Takehiko Ohkawa, Mingfang Zhang, Yifei Huang, Minjie Cai, Ming Li, <span class="under">Ryosuke Furuta</span>, and Yoichi Sato<br>
                <strong>SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training</strong><br>
                <i>ICLR, 2025.</i>
            </li>
        </ul>
        <ul>
            <li>
                Ryoga Takahashi, Yota Yamamoto, <span class="under">Ryosuke Furuta</span>, and Yukinobu Taniguchi<br>
                <strong>Detection of Door-Closing Defects by Learning from Physics-Based Simulations</strong><br>
                <i>VISAPP, 2025.</i>
            </li>
        </ul>
        <ul>
            <li>
                Takehiko Ohkawa, Takuma Yagi, Taichi Nishimura, <span class="under">Ryosuke Furuta</span>, Atsushi Hashimoto, Yoshitaka Ushiku, and Yoichi Sato<br>
                <strong>Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos</strong><br>
                <i>WACV, 2025.</i>
            </li>
        </ul>
        <ul>
            <li>
                Masatoshi Tateno, Takuma Yagi, <span class="under">Ryosuke Furuta</span>, and Yoichi Sato<br>
                <strong>Learning Multiple Object States from Actions via Large Language Models</strong><br>
                <i>WACV, 2025.</i>
            </li>
        </ul>
        <ul>
            <li>
                Takara Taniguchi, and <span class="under">Ryosuke Furuta</span><br>
                <strong>Learning Gaussian Data Augmentation in Feature Space for One-shot Object Detection in Manga</strong><br>
                <i>ACM Multimedia Asia, 2024.</i>
            </li>
        </ul>
        <ul>
            <li>
                Liangyang Ouyang, Ruicong Liu, Yifei Huang, <span class="under">Ryosuke Furuta</span>, and Yoichi Sato<br>
                <strong>ActionVOS: Action as Prompts for Video Object Segmentation</strong><br>
                <i>ECCV, 2024.</i>
            </li>
        </ul>
        <ul>
            <li>
                Takehiko Ohkawa, Takuma Yagi, Taichi Nishimura, <span class="under">Ryosuke Furuta</span>, Atsushi Hashimoto, Yoshitaka Ushiku, and Yoichi Sato<br>
                <strong>Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos</strong><br>
                <i>LPVL 2024 at CVPR 2024.</i>
            </li>
        </ul>
        <ul>
            <li>
                Masatoshi Tateno, Takuma Yagi, <span class="under">Ryosuke Furuta</span>, and Yoichi Sato<br>
                <strong>Learning Object States from Actions via Large Language Models</strong><br>
                <i>LPVL 2024 at CVPR 2024.</i>
            </li>
        </ul>
        <ul>
            <li>
                Kristen Grauman, Andrew Westbury, Lorenzo Torresani, ... , <span class="under">Ryosuke Furuta</span>, ... , and Michael Wray<br>
                <strong>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</strong><br>
                <i>CVPR, 2024.</i>
            </li>
        </ul>
        <ul>
            <li>
                Ryoga Takahashi, Yota Yamamoto, <span class="under">Ryosuke Furuta</span>, and Yukinobu Taniguchi<br>
                <strong>Detection of Door Closing Defects by Analyzing Video from a High-speed Camera</strong><br>
                <i>IW-FCV, short paper, 2024. (Best Poster Award)</i>
            </li>
        </ul>
        <ul>
            <li>
                Yuan Yin, Yifei Huang, <span class="under">Ryosuke Furuta</span>, and Yoichi Sato<br>
                <strong>Proposal-based Temporal Action Localization with Point-level Supervision</strong><br>
                <i>BMVC, 2023.</i>
            </li>
        </ul>
        <ul>
            <li>
                Takuma Yagi, Misaki Ohashi, Yifei Huang, <span class="under">Ryosuke Furuta</span>, Shungo Adachi, Toutai Mitsuyama, and Yoichi Sato<br>
                <strong>FineBio: A Fine-Grained Video Dataset of Biological Experiments with Hierarchical Annotations</strong><br>
                <i>EPIC 2023 at CVPR 2023.</i>
            </li>
        </ul>
        <ul>
            <li>
                Ken Kariya, Go Irie, <span class="under">Ryosuke Furuta</span>, Yota Yamamoto, Shin Aoki, and Yukinobu Taniguchi<br>
                <strong>Point Cloud Based Deep Molecular Pose Estimation for Structure-Based Virtual Screening</strong><br>
                <i>IW-FCV, 2023.</i> (Best Paper Award)
            </li>
        </ul>
        <ul>
            <li>
                Xinyun Li, <span class="under">Ryosuke Furuta</span>, Go Irie, Yota Yamamoto, and Yukinobu Taniguchi<br>
                <strong>Interactive Indoor Localization Based on Image Retrieval and Question Response</strong><br>
                <i>VISAPP, 2023.</i>
            </li>
        </ul>
        <ul>
            <li>
                Zecheng Yu, Yifei Huang, <span class="under">Ryosuke Furuta</span>, Takuma Yagi, Yusuke Gotsu, and Yoichi Sato<br>
                <strong>Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos</strong><br>
                <i>WACV, 2023.</i>
            </li>
        </ul>
        <ul>
            <li>
                Takehiko Ohkawa, Yu-Jhe Li, Qichen Fu, <span class="under">Ryosuke Furuta</span>, Kris Kitani, and Yoichi Sato<br>
                <strong>Domain Adaptive Hand Keypoint and Pixel Localization in the Wild</strong><br>
                <i>ECCV, 2022.</i>
            </li>
        </ul>
        <ul>
            <li>
                Koya Tango, Takehiko Ohkawa, <span class="under">Ryosuke Furuta</span>, and Yoichi Sato<br>
                <strong>Background Mixup Data Augmentation for Hand and Object-in-Contact Detection</strong><br>
                <i>HANDS 2022 at CVPR 2022.</i>
            </li>
        </ul>
        <ul>
            <li>
                Zecheng Yu, Yifei Huang, <span class="under">Ryosuke Furuta</span>, Takuma Yagi, Yusuke Gotsu, and Yoichi Sato<br>
                <strong>Precise Affordance Annotation for Egocentric Action Video Datasets</strong><br>
                <i>EPIC 2022 at CVPR 2022.</i>
            </li>
        </ul>
        <ul>
            <li>
                Delong Ouyang, <span class="under">Ryosuke Furuta</span>, Yugo Shimizu, Yukinobu Taniguchi, Ryota Hinami, and Shonosuke Ishiwatari<br>
                <strong>Interactive Manga Colorization with Fast Flat Coloring</strong><br>
                <i>SIGGRAPH ASIA Posters, 2021.</i>
            </li>
        </ul>
        <ul>
            <li>
                Xinyun Li, <span class="under">Ryosuke Furuta</span>, Go Irie, and Yukinobu Taniguchi<br>
                <strong>Accurate Indoor Localization using Multi-view Image Distance</strong><br>
                <i>IEVC, 2021.</i> (Best Student Paper Award)
            </li>
        </ul>
        <ul>
            <li>
                Risako Ii, <span class="under">Ryosuke Furuta</span>, and Yukinobu Taniguchi<br>
                <strong>Distortion Correction and Stitching of Overlapping Cattle Barn Images</strong><br>
                <i>IEVC, 2021.</i>
            </li>
        </ul>
        <ul>
            <li>
                Yugo Shimizu, <span class="under">Ryosuke Furuta</span>, Delong Ouyang, Yukinobu Taniguchi, Ryota Hinami, and Shonosuke Ishiwatari<br>
                <strong>Painting Style-Aware Manga Colorization Based on Generative Adversarial Networks</strong><br>
                <i>IEEE ICIP, 2021.</i> [<a href="https://arxiv.org/abs/2107.07943">arXiv</a>]
            </li>
        </ul>
        <ul>
            <li>
                Shunta Komatsu, <span class="under">Ryosuke Furuta</span>, and Yukinobu Taniguchi<br>
                <strong>Passenger Flow Estimation with Bipartite Matching on Bus Surveillance Cameras</strong><br>
                <i>IEEE MIPR, 2021.</i>
            </li>
        </ul>
        <ul>
            <li>
                Masaki Sugimoto, <span class="under">Ryosuke Furuta</span>, and Yukinobu Taniguchi<br>
                <strong>Weakly-Supervised Human-Object Interaction Detection</strong><br>
                <i>VISAPP, 2021.</i>
            </li>
        </ul>
        <ul>
            <li>
                <span class="under">Ryosuke Furuta</span>, Naoaki Noguchi, Xueting Wang, and Toshihiko Yamasaki<br>
                <strong>Feature Point Matching in Cross-Spectral Images with Cycle Consistency Learning</strong><br>
                <i>ICPR, 2020.</i>
            </li>
        </ul>
        <ul>
            <li>
                <span class="under">Ryosuke Furuta</span>, Naoto Inoue, and Toshihiko Yamasaki<br>
                <strong>Fully Convolutional Network with Multi-Step Reinforcement Learning for Image Processing</strong><br>
                <i>AAAI, 2019.</i> (acceptance rate 16.2%) [<a href="https://arxiv.org/abs/1811.04323">arXiv</a>] [<a href="https://aaai.org/ojs/index.php/AAAI/article/view/4240">proceedings</a>] [<a href="./pub/fcn_rl/fcn_rl.html">project page</a>] [<a href="https://github.com/rfuruta/pixelRL">code</a>]
            </li>
        </ul>
        <ul>
            <li>
                Naoto Inoue, <span class="under">Ryosuke Furuta</span>, Toshihiko Yamasaki, and Kiyoharu Aizawa<br>
                <strong>Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation</strong><br>
                <i>IEEE CVPR, 2018.</i> (acceptance rate 29.1%) [<a href="https://arxiv.org/abs/1803.11365">pdf</a>] [<a href="https://naoto0804.github.io/cross_domain_detection/">project page</a>] [<a href="https://github.com/naoto0804/cross-domain-detection">code</a>]
            </li>
        </ul>
        <ul>
            <li>
                <span class="under">Ryosuke Furuta</span>, Naoto Inoue, and Toshihiko Yamasaki<br>
                <strong>Efficient and Interactive Spatial-Semantic Image Retrieval</strong><br>
                <i>MMM, 2018.</i> [<a href="./pub/retrieval/llncs.pdf">pdf</a>]
            </li>
        </ul>
        <ul>
            <li>
                Sijie Shen, <span class="under">Ryosuke Furuta</span>, Toshihiko Yamasaki and Kiyoharu Aizawa<br>
                <strong>Fooling Neural Networks in Face Attractiveness Evaluation: Adversarial Examples with High Attractiveness Score but Low Subjective Score</strong><br>
                <i>IEEE BigMM (short paper), 2017.</i>
            </li>
        </ul>
        <ul>
            <li>
                Naoto Inoue, <span class="under">Ryosuke Furuta</span>, Toshihiko Yamasaki and Kiyoharu Aizawa<br>
                <strong>Object Detection Refinement Using Markov Random Field Based Pruning and Learning Based Rescoring</strong><br>
                <i>IEEE ICASSP, 2017.</i> [<a href="http://www.hal.t.u-tokyo.ac.jp/~inoue/pub/icassp2017/icassp2017.pdf">pdf</a>]
            </li>
        </ul>
        <ul>
            <li>
                <span class="under">Ryosuke Furuta</span>, Ikuko Tsubaki, and Toshihiko Yamasaki<br>
                <strong>Fast Volume Seam Carving with Multi-pass Dynamic Programming</strong><br>
                <i>IEEE ICIP, 2016.</i> [<a href="./pub/vsc/icip2016.pdf">pdf</a>] [<a href="./pub/vsc/icip2016.mp4">video</a>]
            </li>
        </ul>
        <ul>
            <li>
                <span class="under">Ryosuke Furuta</span>, Yusuke Fukushima, Toshihiko Yamasaki, and Kiyoharu Aizawa<br>
                <strong>Multi-Label Classification using Class Relations Based on Higher-Order MRF Optimization</strong><br>
                <i>IEEE CVPRW (BigVision), 2016.</i> [<a href="./pub/mrf/bigvision2016.pdf">pdf</a>]
            </li>
        </ul>
        <ul>
            <li>
                Toshihiko Yamasaki, Yusuke Fukushima, <span class="under">Ryosuke Furuta</span>, and Kiyoharu Aizawa<br>
                <strong>Towards Online Prediction of Oral Presentation</strong><br>
                <i>IEEE BigMM-ACM, 2016.</i>
            </li>
        </ul>
        <ul>
            <li>
                Toshihiko Yamasaki, Yusuke Fukushima, <span class="under">Ryosuke Furuta</span>, Litian Sun, Kiyoharu Aizawa, and Danushka Bollegala<br>
                <strong>Prediction of User Ratings of Oral Presentations using Label Relations</strong><br>
                <i>ACMMM-ASM, 2015.</i>
            </li>
        </ul>
        <ul>
            <li>
                <span class="under">Ryosuke Furuta</span>, Satoshi Ikehata, Toshihiko Yamasaki, and Kiyoharu Aizawa<br>
                <strong>Coarse-to-Fine Strategy for Efficient Cost-Volume Filtering</strong><br>
                <i>IEEE ICIP, 2014.</i> (Top 10% Paper Award and IEEE SPS Japan Student Paper Award) [<a href="./pub/c2fcvf/icip2014.pdf">pdf</a>]
            </li>
        </ul>
        <h4>Domestic conferences</h4>
        <ul>
            <li> 88 papers </li>
        </ul>
        <h4>Awards</h4>
        <ul>
            <li>MIRU Paper Evaluation Contribution Award in The Meeting on Image Recognition and Understanding (MIRU) 2024.</li>
            <li>MIRU Audience Award in The Meeting on Image Recognition and Understanding (MIRU) 2024.</li>
            <li>Best Poster Award in The International Workshop on Frontiers of Computer Vision (IW-FCV) 2024. (co-author)</li>
            <li>Best Paper Award in The International Workshop on Frontiers of Computer Vision (IW-FCV) 2023. (co-author)</li>
            <li>Outstanding Presentation Award in Expressive Japan 2022. (three papers, co-author)</li>
            <li>Best Student Paper Award in IIEEJ International Conference on Image Electronics and Visual Computing (IEVC) 2021. (co-author)</li>
            <li>IIEEJ Research Encouragement Award, 2020. (co-author)</li>
            <li>IEEE Signal Processing Society (SPS) Japan Young Author Best Paper Award, 2020.</li>
            <li>IE Special Award, Technical Committee on Image Engineering, Institute of Electronics, Information and Communication Engineers, 2018.</li>
            <li>IE Award, Technical Committee on Image Engineering, Institute of Electronics, Information and Communication Engineers, 2018.</li>
            <li>Telecom System Technology Award, The Telecommunications Advancement Foundation, 2018.</li>
            <li>IE Award, Technical Committee on Image Engineering, Institute of Electronics, Information and Communication Engineers, 2015.</li>
            <li>IMPS Best and Frontier Paper Award, 2015. (co-author)</li>
            <li>IEEE Signal Processing Society (SPS) Japan Student Conference Paper Award, 2015.</li>
            <li>Harashima Hiroshi Science Award, Association for Promotion of Electrical, Electronic and Information Engineering, 2015.</li>
            <li>Top 10% Paper Award in IEEE International Conference on Image Processing (ICIP) 2014.</li>
            <li>ITE Outstanding Student Release Award, The Institute of Image Information and Television Engineers, 2013.</li>
            <li>ITE Suzuki Memorial Young Researcher's Award, The Institute of Image Information and Television Engineers, 2013.</li>
        </ul>
        <h4>Invited Talks</h4>
        <ul>
            <li>
                <strong>Fast Image Processing with Multi-pass Dynamic Programming</strong><br>
                <i>Image Processing Tokyo</i>, Tokyo Institute of Technology, Jan. 2017.
            </li>
        </ul>

        <h3 id="others">Funding sources</h3>
        <ul>
            <li>Grant-in-Aid for Early-Career Scientists, Japan Society for the Promotion of Science (JSPS). (FY2023 - FY2025)</li>
            <li>Research grant, Kayamori Foundation of Informational Science Advancement. (FY2022 - FY2024)</li>
            <li>Grant-in-Aid for Early-Career Scientists, Japan Society for the Promotion of Science (JSPS). (FY2021 - FY2022)</li>
            <li>Grant-in-Aid for Research activity start-up, Japan Society for the Promotion of Science (JSPS). (FY2019 - FY2020)</li>
            <li>Research fellowship for young scientists (DC1), Japan Society for the Promotion of Science (JSPS). (FY2016 - FY2018)</li>
            <li>ITE Financial Support to International Conference Student Release, The Institute of Image Information and Television Engineers, 2014. </li>
        </ul>

        <h3 id="link">Links</h3>      
        <dl>
            <dt><a href="https://www.ut-vision.org/">Sato Laboratory/Sugano Laboratory</a></dt>
            <dt><a href="https://www.iis.u-tokyo.ac.jp/en/">Institute of Industrial Science, The University of Tokyo</a></dt><br>

            <dt><a href="https://www.rs.tus.ac.jp/vml/">Taniguchi Laboratory</a></dt>
            <dt><a href="https://www.tus.ac.jp/en/">Tokyo University of Science</a></dt><br>

            <dt><a href="http://www.cvm.t.u-tokyo.ac.jp/en/">Yamasaki Laboratory</a></dt>
            <dt><a href="http://www.i.u-tokyo.ac.jp/index_e.shtml">Graduate School of Information Science and Technology</a></dt>	
            <dt><a href="http://www.u-tokyo.ac.jp/en/">The University of Tokyo</a></dt>
        </dl>

        <hr>
        <footer>
            <p>© Ryosuke Furuta 2022</p>
        </footer>
    </div><!-- container-narrow -->
    <script src="./furuta_files/bootstrap.js"></script>

</body>
</html>
